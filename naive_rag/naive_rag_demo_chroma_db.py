"""
Naive RAG - –ë–∞–∑–æ–≤–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è
=============================
–ü—Ä–æ—Å—Ç–∞ —ñ–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—è RAG –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º TF-IDF —Ç–∞ –∫–æ—Å–∏–Ω—É—Å–Ω–æ—ó –ø–æ–¥—ñ–±–Ω–æ—Å—Ç—ñ.
–ó –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º ChromaDB –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —á–∞–Ω–∫—ñ–≤ —Ç–∞ –º–µ—Ç–∞–¥–∞–Ω–∏—Ö.

–¢–æ—á–Ω—ñ—Å—Ç—å: ~30% –Ω–∞ —Å–∫–ª–∞–¥–Ω–∏—Ö –∑–∞–ø–∏—Ç–∞—Ö
"""
import sys
import os
from pathlib import Path
import time
import numpy as np
import hashlib
from typing import List, Dict, Optional
import chromadb
from chromadb.config import Settings
from dotenv import load_dotenv

# –î–æ–¥–∞—î–º–æ —à–ª—è—Ö –¥–æ —É—Ç–∏–ª—ñ—Ç
sys.path.append(str(Path(__file__).parent.parent))

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–º—ñ–Ω–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ –∑ .env (—à—É–∫–∞—î–º–æ –≤ –ø–æ—Ç–æ—á–Ω—ñ–π —Ç–∞ –±–∞—Ç—å–∫—ñ–≤—Å—å–∫—ñ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó)
load_dotenv()  # –°–ø–æ—á–∞—Ç–∫—É –ø–æ—Ç–æ—á–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è
if not os.getenv('OPENAI_API_KEY'):
    # –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏, —à—É–∫–∞—î–º–æ –≤ –±–∞—Ç—å–∫—ñ–≤—Å—å–∫—ñ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó
    load_dotenv(Path(__file__).parent.parent / '.env')

from utils.data_loader import DocumentLoader, TextSplitter, save_results, print_results


def generate_answer_with_llm(question: str, contexts: List[str], max_tokens: int = 256) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —á–µ—Ä–µ–∑ LLM
    –°–ø—Ä–æ–±–∞ 1: Ollama (–ª–æ–∫–∞–ª—å–Ω–æ, –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–æ)
    –°–ø—Ä–æ–±–∞ 2: OpenAI (—è–∫—â–æ —î API key), –∑—Ä–æ–±—ñ—Ç—å export OPENAI_API_KEY=your_key
    –°–ø—Ä–æ–±–∞ 3: Simple fallback - –ø–æ–≤–µ—Ä–Ω—É—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç
    """
    # –°–ø—Ä–æ–±–∞ 1: Ollama (–ª–æ–∫–∞–ª—å–Ω–æ)
    try:
        import requests
        prompt = f"Based on the following context, answer the question.\n\nContext:\n{chr(10).join(contexts[:3])}\n\nQuestion: {question}\n\nAnswer:"

        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "llama3.2:3b",
                "prompt": prompt,
                "stream": False,
                "options": {"temperature": 0.7, "num_predict": max_tokens}
            },
            timeout=30
        )

        if response.status_code == 200:
            return response.json()["response"].strip()
    except Exception:
        pass

    # –°–ø—Ä–æ–±–∞ 2: OpenAI (—è–∫—â–æ —î API key)
    # –î–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è: export OPENAI_API_KEY=your_key
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            from openai import OpenAI
            client = OpenAI(api_key=api_key)

            prompt = f"Based on the following context, answer the question.\n\nContext:\n{chr(10).join(contexts[:3])}\n\nQuestion: {question}"

            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ OpenAI: {e}")
        pass

    # –°–ø—Ä–æ–±–∞ 3: Fallback - –ø—Ä–æ—Å—Ç–æ –ø–æ–≤–µ—Ä–Ω—É—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç
    return "\n\n".join(contexts[:3]) if contexts else "–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó."


def detect_llm_provider() -> str:
    """–í–∏–∑–Ω–∞—á–∞—î —è–∫–∏–π LLM provider –¥–æ—Å—Ç—É–ø–Ω–∏–π"""
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ Ollama
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        if response.status_code == 200:
            return "ollama (llama3.2:3b)"
    except:
        pass

    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ OpenAI
    if os.getenv("OPENAI_API_KEY"):
        return "openai (gpt-4o-mini)"

    return "fallback (–±–µ–∑ LLM)"


def compute_file_checksum(file_path: str) -> str:
    """
    –û–±—á–∏—Å–ª—é—î SHA256 —á–µ–∫—Å—É–º—É —Ñ–∞–π–ª—É –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –∑–º—ñ–Ω.
    
    Args:
        file_path: –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É
        
    Returns:
        SHA256 —á–µ–∫—Å—É–º–∞ —É –≤–∏–≥–ª—è–¥—ñ hex —Ä—è–¥–∫–∞
    """
    sha256_hash = hashlib.sha256()
    with open(file_path, "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()


class SimpleEmbeddings:
    """
    –ü—Ä–æ—Å—Ç–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—ó.
    –ù–µ –ø–æ—Ç—Ä–µ–±—É—î –∑–æ–≤–Ω—ñ—à–Ω—ñ—Ö API - –ø—Ä–∞—Ü—é—î –Ω–∞ numpy.
    """

    def __init__(self):
        self.vocabulary = {}
        self.idf = {}

    def fit(self, documents: List[str]):
        """–ë—É–¥—É—î —Å–ª–æ–≤–Ω–∏–∫ —Ç–∞ —Ä–æ–∑—Ä–∞—Ö–æ–≤—É—î IDF –∑–Ω–∞—á–µ–Ω–Ω—è"""
        # –ë—É–¥—É—î–º–æ —Å–ª–æ–≤–Ω–∏–∫ –∑—ñ –≤—Å—ñ—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤
        doc_word_sets = []
        for doc in documents:
            words = set(doc.lower().split())
            doc_word_sets.append(words)
            for word in words:
                self.vocabulary[word] = self.vocabulary.get(word, 0) + 1

        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ IDF (Inverse Document Frequency) –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–ª–æ–≤–∞
        num_docs = len(documents)
        for word in self.vocabulary:
            doc_count = sum(1 for word_set in doc_word_sets if word in word_set)
            self.idf[word] = np.log(num_docs / (doc_count + 1))

    def embed(self, text: str) -> np.ndarray:
        """–°—Ç–≤–æ—Ä—é—î TF-IDF –≤–µ–∫—Ç–æ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç—É"""
        words = text.lower().split()
        word_count = {}
        for word in words:
            word_count[word] = word_count.get(word, 0) + 1

        # –ë—É–¥—É—î–º–æ TF-IDF –≤–µ–∫—Ç–æ—Ä
        vector = np.zeros(len(self.vocabulary))
        for i, word in enumerate(sorted(self.vocabulary.keys())):
            if word in word_count:
                tf = word_count[word] / len(words)
                idf = self.idf.get(word, 0)
                vector[i] = tf * idf

        return vector

    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """–†–æ–∑—Ä–∞—Ö–æ–≤—É—î –∫–æ—Å–∏–Ω—É—Å–Ω—É –ø–æ–¥—ñ–±–Ω—ñ—Å—Ç—å –º—ñ–∂ –¥–≤–æ–º–∞ –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)


class NaiveRAG:
    """
    –ë–∞–∑–æ–≤–∞ RAG —Å–∏—Å—Ç–µ–º–∞ –∑ —Ç—Ä—å–æ–º–∞ –æ—Å–Ω–æ–≤–Ω–∏–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏:
    1. –†–æ–∑–±–∏—Ç—Ç—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –Ω–∞ —á–∞–Ω–∫–∏
    2. TF-IDF –≤–µ–∫—Ç–æ—Ä–Ω–∏–π –ø–æ—à—É–∫
    3. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —á–µ—Ä–µ–∑ LLM
    
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î ChromaDB –¥–ª—è persistent –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —á–∞–Ω–∫—ñ–≤ —Ç–∞ –º–µ—Ç–∞–¥–∞–Ω–∏—Ö.
    """

    def __init__(
        self, 
        documents_path: str = "data/pdfs", 
        chunk_size: int = 500, 
        chunk_overlap: int = 100,
        chromadb_path: str = "naive_rag_chromadb"
    ):
        self.documents_path = documents_path
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.chunks = []
        self.chunk_embeddings = []
        self.embeddings_model = SimpleEmbeddings()
        
        # ChromaDB –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —á–∞–Ω–∫—ñ–≤
        self.chroma_client = None
        self.collection = None
       
        self._init_chromadb(Path(__file__).parent.parent / chromadb_path)

    def _init_chromadb(self, chromadb_path: str):
        try:
            # –°—Ç–≤–æ—Ä—é—î–º–æ persistent –∫–ª—ñ—î–Ω—Ç
            self.chroma_client = chromadb.PersistentClient(
                path=str(chromadb_path)
            )

            # –°—Ç–≤–æ—Ä—é—î–º–æ –∞–±–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ collection
            try:
                self.collection = self.chroma_client.get_collection(name="naive_rag_chunks")
                print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —ñ—Å–Ω—É—é—á—É ChromaDB collection")
            except:
                self.collection = self.chroma_client.create_collection(
                    name="naive_rag_chunks",
                    metadata={"description": "Naive RAG document chunks with TF-IDF"}
                )
                print("‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ –Ω–æ–≤—É ChromaDB collection")
        except Exception as e:
            print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó ChromaDB: {e}")
            exit(1)

    def _is_document_chunks_exists(self, source: str, file_checksum: str, chunk_size: int, chunk_overlap: int) -> bool:
        """
        –ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ —î –≤–∂–µ —á–∞–Ω–∫–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ ChromaDB.
        
        Args:
            source: –Ü–º'—è —Ñ–∞–π–ª—É (source)
            
        Returns:
            True —è–∫—â–æ —î —á–∞–Ω–∫–∏, False —è–∫—â–æ –Ω–µ–º–∞—î
        """
        if not self.collection:
            return False

        try:
            # –®—É–∫–∞—î–º–æ –≤—Å—ñ —á–∞–Ω–∫–∏ –¥–ª—è —Ü—å–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            results = self.collection.get(
                where={"$and": [{"source": source}, {"chunk_size": str(chunk_size)}, {"chunk_overlap": str(chunk_overlap)}, {"file_checksum": file_checksum}]},
            )

            if results['ids']:
                return True
        except Exception as e:
            print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —ñ—Å–Ω—É—é—á–∏—Ö —á–∞–Ω–∫—ñ–≤ –¥–ª—è {source}: {e}")
        
        return False

    def _delete_chunks_by_source(self, source: str, file_checksum: str):
        """–í–∏–¥–∞–ª—è—î –≤—Å—ñ —á–∞–Ω–∫–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∑ ChromaDB"""

        try:
            # –û—Ç—Ä–∏–º—É—î–º–æ –≤—Å—ñ ID —á–∞–Ω–∫—ñ–≤ –¥–ª—è —Ü—å–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            results = self.collection.get(
                where={"source": source}
            )

            if results['ids']:
                for i, id in enumerate(results['ids']):
                    if results['metadatas'][i]['file_checksum'] != file_checksum:
                        self.chroma_client.delete_collection(id)
                        print(f"  üóëÔ∏è  –í–∏–¥–∞–ª–µ–Ω–æ —á–∞–Ω–∫ {id} –¥–ª—è {source} —á–µ—Ä–µ–∑ –∑–º—ñ–Ω—É —á–µ–∫—Å—É–º–∏ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª—É")
        except Exception as e:
            print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è —á–∞–Ω–∫—ñ–≤ –¥–ª—è {source}: {e}")

    def load_and_process_documents(self, max_documents=None):
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î PDF —Ñ–∞–π–ª–∏ —Ç–∞ —Ä–æ–∑–±–∏–≤–∞—î –Ω–∞ —á–∞–Ω–∫–∏.
        –ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ –≤–∂–µ —î —á–∞–Ω–∫–∏ –≤ ChromaDB –∑ —Ç–∞–∫–∏–º–∏ –∂ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Ç–∞ —á–µ–∫—Å—É–º–æ—é.
        –ù–µ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î –≤—Å—ñ —á–∞–Ω–∫–∏ –∑ –ë–î –≤ –ø–∞–º'—è—Ç—å - —Ç—ñ–ª—å–∫–∏ –∑–±–µ—Ä—ñ–≥–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –¥–∂–µ—Ä–µ–ª–∞.
        """
        loader = DocumentLoader(self.documents_path)
        documents = loader.load_documents(max_documents=max_documents)

        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–∂–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –≤ ChromaDB
        documents_to_process = []

        for doc in documents:
            source = doc["source"]
            file_path = doc.get("path", "")
            
            # –û–±—á–∏—Å–ª—é—î–º–æ —á–µ–∫—Å—É–º—É —Ñ–∞–π–ª—É
            if file_path and Path(file_path).exists():
                current_checksum = compute_file_checksum(file_path)
            else:
                current_checksum = ""

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î –≤–∂–µ —á–∞–Ω–∫–∏ –≤ –ë–î
            self._delete_chunks_by_source(source, current_checksum)
            _is_document_chunks_exists = self._is_document_chunks_exists(source, current_checksum, self.chunk_size, self.chunk_overlap)

            if _is_document_chunks_exists:
                print(f"  ‚úì {source}: —á–∞–Ω–∫–∏ –≤–∂–µ —î –≤ –ë–î")
                continue
            
            documents_to_process.append(doc)

        if documents_to_process:
            print(f"  üìù –ü–æ—Ç—Ä—ñ–±–Ω–æ –æ–±—Ä–æ–±–∏—Ç–∏ {len(documents_to_process)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤")

        # –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ —á–∞–Ω–∫–∏ —Ç—ñ–ª—å–∫–∏ –Ω–æ–≤—ñ/–∑–º—ñ–Ω–µ–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏
        new_chunks = []
        if documents_to_process:
            splitter = TextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)
            new_chunks = splitter.split_documents(documents_to_process)
            
            # –î–æ–¥–∞—î–º–æ —á–µ–∫—Å—É–º—É –¥–æ –º–µ—Ç–∞–¥–∞–Ω–∏—Ö —á–∞–Ω–∫—ñ–≤
            for chunk in new_chunks:
                source = chunk["source"]
                # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
                doc = next((d for d in documents_to_process if d["source"] == source), None)
                if doc:
                    file_path = doc.get("path", "")
                    if file_path and Path(file_path).exists():
                        chunk["file_checksum"] = compute_file_checksum(file_path)
                    else:
                        chunk["file_checksum"] = ""

        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –Ω–æ–≤—ñ —á–∞–Ω–∫–∏ (–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –≤—Å—ñ –∑ –ë–î)
        self.chunks = new_chunks

        if new_chunks:
            print(f"  üìù –ü–æ—Ç—Ä—ñ–±–Ω–æ –∑–±–µ—Ä–µ–≥—Ç–∏ {len(new_chunks)} —á–∞–Ω–∫—ñ–≤")
            self._save_new_chunks_to_db(new_chunks)

        return documents

    def _find_chunks_by_source_any_checksum(self, source: str) -> bool:
        """
        –ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ —î —á–∞–Ω–∫–∏ –¥–ª—è –¥–∂–µ—Ä–µ–ª–∞ –≤ –ë–î (–Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —á–µ–∫—Å—É–º–∏).
        
        Returns:
            True —è–∫—â–æ —î —á–∞–Ω–∫–∏, False —è–∫—â–æ –Ω–µ–º–∞—î
        """
        if not self.collection:
            return False

        try:
            results = self.collection.get(
                where={"source": source},
                limit=1  # –¢—ñ–ª—å–∫–∏ –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å
            )
            return len(results.get('ids', [])) > 0
        except Exception as e:
            return False

    def _load_chunks_from_db(self, source: str, limit: int = None) -> List[Dict]:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —á–∞–Ω–∫–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∑ ChromaDB (lazy loading).
        
        Args:
            source: –Ü–º'—è —Ñ–∞–π–ª—É
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —á–∞–Ω–∫—ñ–≤ (None = –≤—Å—ñ)
        """
        if not self.collection:
            return []

        try:
            # –û—Ç—Ä–∏–º—É—î–º–æ —á–∞–Ω–∫–∏ –¥–ª—è —Ü—å–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            query_limit = limit if limit else 10000
            results = self.collection.get(
                where={"source": source},
                limit=query_limit
            )
            
            chunks = []
            if results['ids']:
                for i, chunk_id in enumerate(results['ids']):
                    metadata = results['metadatas'][i]
                    chunk = {
                        "content": results['documents'][i],
                        "source": metadata.get("source", source),
                        "chunk_id": int(metadata.get("chunk_id", 0)),
                        "total_chunks": int(metadata.get("total_chunks", 0)),
                        "file_checksum": metadata.get("file_checksum", ""),
                        "file_path": metadata.get("file_path", "")
                    }
                    chunks.append(chunk)
                
                # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ chunk_id
                chunks.sort(key=lambda x: x["chunk_id"])
            
            return chunks
        except Exception as e:
            print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —á–∞–Ω–∫—ñ–≤ –∑ –ë–î –¥–ª—è {source}: {e}")
            return []

    def _save_new_chunks_to_db(self, chunks: List[Dict]):
        """
        –ó–±–µ—Ä—ñ–≥–∞—î –Ω–æ–≤—ñ —á–∞–Ω–∫–∏ –≤ ChromaDB.
        """

        # –í–∏–∑–Ω–∞—á–∞—î–º–æ —è–∫—ñ —á–∞–Ω–∫–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–±–µ—Ä–µ–≥—Ç–∏ (—Ç—ñ–ª—å–∫–∏ –Ω–æ–≤—ñ, —â–æ –º–∞—é—Ç—å file_checksum)
        chunks_to_save = []
        for chunk in chunks:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π ID: source + chunk_id
            chunk_id_str = f"{chunk['source']}_chunk_{chunk.get('chunk_id', 0)}"
            chunks_to_save.append((chunk_id_str, chunk))

        if not chunks_to_save:
            return

        # –ì–æ—Ç—É—î–º–æ –¥–∞–Ω—ñ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
        ids = []
        documents = []
        metadatas = []

        for chunk_id_str, chunk in chunks_to_save:
            ids.append(chunk_id_str)
            documents.append(chunk["content"])
            metadatas.append({
                "source": chunk["source"],
                "chunk_id": str(chunk.get("chunk_id", 0)),
                "chunk_size": str(self.chunk_size),
                "chunk_overlap": str(self.chunk_overlap),
                "file_checksum": chunk.get("file_checksum", ""),
            })

        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –±–∞—Ç—á–∞–º–∏
        batch_size = 100
        saved_count = 0
        for i in range(0, len(ids), batch_size):
            batch_ids = ids[i:i+batch_size]
            batch_docs = documents[i:i+batch_size]
            batch_meta = metadatas[i:i+batch_size]

            try:
                self.collection.add(
                    ids=batch_ids,
                    documents=batch_docs,
                    metadatas=batch_meta
                )
                saved_count += len(batch_ids)
            except Exception as e:
                # –ú–æ–∂–ª–∏–≤–æ —á–∞–Ω–∫ –≤–∂–µ —ñ—Å–Ω—É—î - —Ü–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
                if "duplicate" not in str(e).lower():
                    print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —á–∞–Ω–∫—ñ–≤ –≤ –ë–î: {e}")

        if saved_count > 0:
            print(f"  üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ {saved_count} –Ω–æ–≤–∏—Ö —á–∞–Ω–∫—ñ–≤ –≤ ChromaDB")

    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:
        """
        –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –Ω–∞–π–±—ñ–ª—å—à —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ —á–∞–Ω–∫–∏ —á–µ—Ä–µ–∑ –∫–æ—Å–∏–Ω—É—Å–Ω—É –ø–æ–¥—ñ–±–Ω—ñ—Å—Ç—å.
        –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —á–∞–Ω–∫–∏ –∑ –ë–î —Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ –ø–æ—Ç—Ä–µ–±—ñ (lazy loading).

        Args:
            query: –ó–∞–ø–∏—Ç–∞–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
            top_k: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —á–∞–Ω–∫—ñ–≤ –¥–ª—è –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è

        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–æ–ø-k –Ω–∞–π–±—ñ–ª—å—à —Å—Ö–æ–∂–∏—Ö —á–∞–Ω–∫—ñ–≤ –∑ –æ—Ü—ñ–Ω–∫–∞–º–∏
        """
        top_chunks = self.chroma_client.get_collection(name="naive_rag_chunks").query(
            query_texts=[query],
            n_results=top_k,
            where={"$and": [{"chunk_size": str(self.chunk_size)}, {"chunk_overlap": str(self.chunk_overlap)}]},
        )

        return top_chunks

    def generate_answer(self, query: str, context_chunks: List[Dict]) -> str:
        """
        –ì–µ–Ω–µ—Ä—É—î –≤—ñ–¥–ø–æ–≤—ñ–¥—å —á–µ—Ä–µ–∑ LLM –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –∑–Ω–∞–π–¥–µ–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.

        Args:
            query: –ó–∞–ø–∏—Ç–∞–Ω–Ω—è
            context_chunks: –ó–Ω–∞–π–¥–µ–Ω—ñ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ —á–∞–Ω–∫–∏

        Returns:
            –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        """
        if not context_chunks:
            return "–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó."

        # ChromaDB returns nested lists: documents[0] is list of documents for first query
        documents = context_chunks.get("documents", [[]])
        contexts = documents[0] if documents and documents[0] else []

        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
        answer = generate_answer_with_llm(
            question=query,
            contexts=contexts,
            max_tokens=256,
        )

        return answer

    def query(self, question: str, top_k: int = 3) -> Dict:
        """
        –í–∏–∫–æ–Ω—É—î –ø–æ–≤–Ω–∏–π RAG pipeline: –ø–æ—à—É–∫ + –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è.

        Args:
            question: –ó–∞–ø–∏—Ç–∞–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
            top_k: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —á–∞–Ω–∫—ñ–≤ –¥–ª—è –ø–æ—à—É–∫—É

        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é —Ç–∞ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏
        """
        start_time = time.time()

        # –ö—Ä–æ–∫ 1: –ü–æ—à—É–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —á–∞–Ω–∫—ñ–≤
        relevant_chunks = self.retrieve(question, top_k=top_k)

        # –ö—Ä–æ–∫ 2: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
        answer = self.generate_answer(question, relevant_chunks)

        execution_time = time.time() - start_time

        # ChromaDB returns nested lists: distances[0] is list of distances for first query
        distances = relevant_chunks.get("distances", [[]])
        ids = relevant_chunks.get("ids", [[]])
        documents = relevant_chunks.get("documents", [[]])
        
        result = {
            "question": question,
            "answer": answer,
            "relevant_chunks": len(distances[0]) if distances and distances[0] else 0,
            "sources": ids[0] if ids and ids[0] else [],
            "scores": distances[0] if distances and distances[0] else [],
            "contexts": documents[0] if documents and documents[0] else [],
            "execution_time": execution_time
        }

        return result


def run_naive_rag_demo():
    """–ó–∞–ø—É—Å–∫–∞—î –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—é Naive RAG –∑ —Ç–µ—Å—Ç–æ–≤–∏–º–∏ –∑–∞–ø–∏—Ç–∞–º–∏"""
    print("="*70)
    print("NAIVE RAG –î–ï–ú–û–ù–°–¢–†–ê–¶–Ü–Ø")
    print("="*70)

    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∏—Å—Ç–µ–º–∏
    chunk_size = 500
    chunk_overlap = 100
    rag = NaiveRAG(
        documents_path="data/pdfs",
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )

    # –í–∏–≤–æ–¥–∏–º–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
    print(f"\n–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è:")
    llm_model = detect_llm_provider()
    print(f"  –ú–æ–¥–µ–ª—å LLM: {llm_model}")
    print(f"  –†–æ–∑–º—ñ—Ä —á–∞–Ω–∫—É: {chunk_size} —Å–∏–º–≤–æ–ª—ñ–≤")
    print(f"  –ü–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è —á–∞–Ω–∫—ñ–≤: {chunk_overlap} —Å–∏–º–≤–æ–ª—ñ–≤")

    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–æ–∫—É–º–µ–Ω—Ç–∏
    # –ü—Ä–∏–º—ñ—Ç–∫–∞: max_documents=50 –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –¥–µ–º–æ. None - –≤—Å—ñ 660 –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤.
    print(f"\n–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤...")
    documents = rag.load_and_process_documents(max_documents=50)
    print(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤")

    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –£–ù–Ü–§–Ü–ö–û–í–ê–ù–ò–ô —Ç–µ—Å—Ç–æ–≤–∏–π –¥–∞—Ç–∞—Å–µ—Ç (100 –∑–∞–ø–∏—Ç—ñ–≤)
    # –í–ê–ñ–õ–ò–í–û: –í—Å—ñ RAG –ø—ñ–¥—Ö–æ–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –¢–Ü –°–ê–ú–Ü –∑–∞–ø–∏—Ç–∏ –¥–ª—è –∫–æ—Ä–µ–∫—Ç–Ω–æ–≥–æ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è!
    loader = DocumentLoader()
    unified_queries = loader.load_unified_queries(max_queries=50)  # –ü–µ—Ä—à—ñ 50 –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
    print(f"–¢–µ—Å—Ç–æ–≤–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤: {len(unified_queries)}")

    # –ó–∞–ø—É—Å–∫–∞—î–º–æ —Ç–µ—Å—Ç–∏
    print("\n" + "="*70)
    print("–í–ò–ö–û–ù–ê–ù–ù–Ø –¢–ï–°–¢–Ü–í")
    print("="*70)

    all_results = {
        "system_name": "Naive RAG",
        "total_documents": len(documents),
        "total_chunks": len(rag.chunks),
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "llm_model": detect_llm_provider(),
        "queries": []
    }

    # –ì—Ä—É–ø—É—î–º–æ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö –¥–ª—è –≤–∏–≤–æ–¥—É
    from collections import defaultdict
    queries_by_category = defaultdict(list)
    for query in unified_queries:
        queries_by_category[query.get("category", "general")].append(query)

    # –¢–µ—Å—Ç—É—î–º–æ –∑–∞–ø–∏—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö
    for category, queries in queries_by_category.items():
        print(f"\n–ö–∞—Ç–µ–≥–æ—Ä—ñ—è: {category}")

        for query_data in queries:
            question = query_data.get("question", "")

            # –í–∏–∫–æ–Ω—É—î–º–æ –∑–∞–ø–∏—Ç
            result = rag.query(question, top_k=3)
            result["category"] = category
            result["query_id"] = query_data.get("id")
            result["difficulty"] = query_data.get("difficulty")
            all_results["queries"].append(result)

            # –í–∏–≤–æ–¥–∏–º–æ –∫–æ—Ä–æ—Ç–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            print(f"  ID {query_data.get('id')}: {question[:70]}...")
            score = result['scores'][0] if result['scores'] else 0.0
            print(f"  –ß–∞—Å: {result['execution_time']:.2f}—Å | –û—Ü—ñ–Ω–∫–∞: {score:.3f}")

    # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –ø—ñ–¥—Å—É–º–∫–æ–≤—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    avg_time = np.mean([q["execution_time"] for q in all_results["queries"]])
    avg_score = np.mean([q["scores"][0] if q["scores"] else 0.0 for q in all_results["queries"]])

    all_results["metrics"] = {
        "average_execution_time": avg_time,
        "average_top_score": avg_score,
        "total_queries": len(all_results["queries"])
    }

    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    save_results(all_results, "results/naive_rag_chroma_db_results.json")

    # –í–∏–≤–æ–¥–∏–º–æ –ø—ñ–¥—Å—É–º–æ–∫
    print("\n" + "="*70)
    print("–ü–Ü–î–°–£–ú–û–ö")
    print("="*70)
    print(f"–í—Å—å–æ–≥–æ –∑–∞–ø–∏—Ç—ñ–≤: {len(all_results['queries'])}")
    print(f"–°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è: {avg_time:.2f}—Å")
    print(f"–°–µ—Ä–µ–¥–Ω—è –æ—Ü—ñ–Ω–∫–∞: {avg_score:.3f}")
    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: results/naive_rag_chroma_db_results.json")

    print("\n" + "="*70)
    print("–û–±–º–µ–∂–µ–Ω–Ω—è Naive RAG:")
    print("  - –ù–∏–∑—å–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ —Å–∫–ª–∞–¥–Ω–∏—Ö –∑–∞–ø–∏—Ç–∞—Ö (~30%)")
    print("  - –í—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –º—ñ–∂ —á–∞–Ω–∫–∞–º–∏")
    print("  - –ù–µ–º–∞—î –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ")
    print("  - –ü—Ä–æ–±–ª–µ–º–∞ 'Lost in the Middle'")
    print("="*70)


if __name__ == "__main__":
    run_naive_rag_demo()
